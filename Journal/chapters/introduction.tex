\section{Introduction}

\IEEEPARstart{M}{ultiple}
sclerosis (MS) is an inflammatory and demyelinating disease of the central
nervous system with pathology that can be observed in vivo by magnetic resonance
imaging (MRI). MS is characterized by the formation of lesions, primarily
visible in the white matter on conventional MRI. Imaging biomarkers based on the
delineation of lesions, such as lesion load and lesion count, have established
their importance for assessing disease progression and treatment effect.
However, lesions vary greatly in size, shape, intensity and location, which
makes their automatic and accurate segmentation challenging.

% Generative model was dictionary learning and sparse coding. Might extend
% clustering a little bit if need be.

Many automatic methods have been proposed for the segmentation of MS
\mbox{lesions} over the last two decades \cite{garcia2013review}, which can be
classified into unsupervised and supervised methods. Unsupervised methods do not
require a labeled data set for training. Instead, lesions are identified as an
outlier of, e.g., a subject specific generative model of tissue intensities
\cite{vanleemput2001,tomas2015,schmidt2012automated,roura2015}, or a generative
model of image patches representing a healthy population \cite{weiss2013}.
Alternatively, clustering methods have been used to segment healthy and lesion
tissue, where lesions are modelled as a separate tissue class
\cite{shiee2010topology,sudre2015}.
In many methods, spatial priors of healthy tissues are used to reduce false
positives. For example, in addition to modelling MS lesions as a separate
intensity cluster, Lesion-TOADS \cite{shiee2010topology} employs topological and
statistical atlases to produce a topology-preserving segmentation of all brain
tissues.
%, while the expectation-maximization segmentation (EMS)
%\cite{vanleemput2001} method uses a Markov random field (MRF) to produce a
%spatially consistent segmentation.
% TODO: maybe remove or reword the EMS MRF.

% To account for the variability of intensity distributions for different
% subjects and scanning sequences, Sudre et al used Bayesian model selection to
% select the most appropriate statistical model for each healty tissue and
% lesion class.
To account for local changes of the tissue intensity distributions,
Tomas-Fernandez et al. \cite{tomas2015} combined the subject-specific model of
the global intensity distributions with a voxel-specific model calculated from a
healthy population, where lesions are detected as outliers of the combined
model. A major challenge of unsupervised methods is that outliers are often not
specific to lesions and can also be caused by intensity inhomogeneities, partial
volume, imaging artifacts, and small anatomical structures such as blood
vessels, which leads to the generation of false positives. To overcome these
limitations, Roura et al. \cite{roura2015} employed an additional set of rules
%TODO: Can we add an adjective or two to make this more specific? (rules)
to remove false positives, while Schmidt et al. \cite{schmidt2012automated} used
a conservative threshold for the initial detection of lesions, which are later
grown in a separate step to yield an accurate delineation.


% Outlier of a subject-specific generative model of healthy tissue intensities
% [ems] or outlier of image patches representing a healthy population [weiss].
% Or modelled as a separate cluster [shiee,sudre].

%Other methods derive a tissue segmentation using clustering methods and model
%lesions as a separate cluster \cite{shiee2010topology,sudre2015}. The
%intensities of healthy and lesion tissue might can not always be modelled by a
%single centroid or multivariate Gaussian. Sudre et al. have used Bayesian
%inference to select the most appropriate statistical model to represent each
%tissue cluster.

% Instead, lesions are identified as an intensity outlier class using, e.g.,
% clustering methods \cite{shiee2010topology} or generative models
% \cite{weiss2013}. In many methods, spatial priors of healthy tissues are used to
% reduce false positives. For example, in addition to modelling MS lesions as a
% separate cluster, Lesion-TOADS \cite{shiee2010topology} employs a topological
% and a statistical atlas to produce a topology-preserving segmentation of all
% brain tissues.

Current supervised approaches typically start with a set of features, which can
range from small and simple to large and highly variable, and either predefined
by the user \cite{geremia2010,guizard2015,subbanna2015} or gathered in a feature
extraction step such as by deep learning \cite{yoo2014}. Voxel-based
segmentation algorithms \cite{geremia2010,yoo2014} feed the features and labels
of each voxel into a general classification algorithm, such as a random forest
\cite{breiman2001}, to classify each voxel and to determine which set of
features are the most important for segmentation in the particular domain.
% TODO: introduce MRF better
Voxel features and the labels of neighboring voxels can be incorporated into
Markov random field-based (MRF-based) approaches
\cite{subbanna2009,subbanna2015} to produce a spatially consistent segmentation.
As a strategy to reduce false positives, Subbanna et al.
\cite{subbanna2015} combined a voxel-level MRF with a regional MRF, which
integrates a large set of intensity and textural features extracted from the
regions produced by the voxel-level MRF with the labels of neighboring nodes of
the regional MRF.
Library-based approaches leverage a library of pre-segmented images to carry out
the segmentation. For example, Guizard et al.
\cite{guizard2015} proposed a segmentation method based on an extension of the
non-local means algorithms \cite{coupe2011}. The centers of patches at every
voxel location are classified based on matched patches from a library containing
pre-segmented images, where multiple matches are weighted using a similarity
measure based on rotation-invariant features.

% Current supervised approaches typically start with a simple or large set of
% features, either predefined by the user
% \cite{geremia2010,guizard2015,subbanna2015} or gathered in a feature extraction
% step such as by deep learning \cite{yoo2014}, which is followed by a separate
% training step with labeled data to determine which set of features are the most
% important for segmentation in the particular domain. Alternative approaches were
% proposed by Subbanna et al. and Guizard et al.
% 
% Alternative, start with a simple set of features and derives more complex
% features from regions created by an initial segmentation. \cite{subbanna2015}:
% Combines two MRFs. Initial segmentation carried out by the voxel level MRF that
% uses the image intensities and intensity differences as input features.
% Segmentation is iteratively updated using a regional MRF, where each node uses a
% complex set of intensity and textual feature derived from the regions segmented
% by the voxel level MRF. Distributions of features for each tissue type and
% transitions are learned from training data.
% 
% Guizard et al. \cite{guizard2015} proposed a segmentation method based on an
% extension of the non-local means algorithms \cite{coupe2011}. The centers of a
% patch at every voxel location is classified based on matched patches from a
% library containing pre-segmented images, where multiple matches are weighted
% using a similarity measure based on rotation-invariant features extracted from
% the patches.

% For example, Yoo et al. \cite{yoo2014} proposed performing unsupervised
% learning of domain-specific features from image patches from unlabelled data
% using deep learning.
A recent breakthrough for automatic segmentation using deep learning comes from
the domain of cell membrane segmentation, in which Cire\c{s}an et al.
\cite{Ciresan2012} proposed classifying the centers of image patches directly
using a convolutional neural network (CNN) \cite{LeCun1998} without a dedicated
feature extraction step. Instead, features are learned indirectly within the
lower layers of the neural network during training, while the higher layers can
be regarded as performing the classification, which allows the learning of
features that are specifically tuned to the segmentation task. However, the time
required to train patch-based methods can make the approach infeasible when the
size and number of patches are large.

% TODO: can I add numerical values to it? Mostly my personal experience.

% TODO: Where does this approach fit in, what is the contribution?

Recently, different CNN architectures
\cite{long2015,ronneberger2015,brosch2015,kang2014fully} have been proposed that
are able to feed through entire images, which removes the need to select
representative patches, eliminates redundant calculations where patches overlap,
and therefore these models scale up more efficiently with image resolution. Kang
et al. introduced the fully convolutional neural network (fCNN) for the
segmentation of crowds in surveillance videos \cite{kang2014fully}. However, fCNNs produce
segmentations of lower resolution than the input images due to the successive
use of convolutional and pooling layers, both of which reduce the
dimensionality.
To predict segmentations of the same resolution as the input images, we recently
proposed using a 3-layer convolutional encoder network (CEN) \cite{brosch2015}
for MS lesion segmentation. The combination of convolutional \cite{LeCun1998}
and deconvolutional \cite{zeiler2011} layers allows our network to produce
segmentations that are of the same resolution as the input images.

Another limitation of the traditional CNN is the trade-off between localization
accuracy, represented by lower-level features, and contextual information,
provided by higher-level features. To overcome this limitation, Long et al.
\cite{long2015} proposed fusing the segmentations produced by the lower layers of the network
with the upsampled segmentations produced by higher layers. However, using only
low-level features was not sufficient to produce a good segmentation at the
lowest layers, which is why segmentation fusing was only performed for the three
highest layers.
% TODO: This is unclear. What was wrong with the low-level segmentations? I can
% imagine too many false positives?
Instead of combining the segmentations produced at
different layers, Ronneberger et al. \cite{ronneberger2015} proposed combining
the features of different layers to calculate the final segmentation
directly at the lowest layer using an 11-layer u-shaped network architecture
called u-net. Their network is
% which leverages high-level and low-level features in order to predict
% segmentations that are both robust and accurate.
composed of a traditional contracting path (first half of the u), but augmented
with an expanding path (last half of the u), which replaces the pooling layers
of the contracting path with upsampling operations. To leverage both high- and
low-level features, shortcut connections are added between corresponding layers
of the two paths.
However,
% the successive application of convolutional, pooling, and unpooling layers
% reduces the size of the predicted segmentation by the size of the receptive
% field. This requires special handling of the border regions and limits the
% maximum size of the receptive field.
upsampling cannot fully compensate for the loss of resolution, and special
handling of the border regions is still required.

% Easier to apply than u-net because it does not need padding or cropping of the
% segmentation for training.
% 
% In this paper, we extend our previous framework to include more layers. Our
% model consists of two pathways, a convolutional pathway which consists of
% alternating convolutional and pooling layers and learns increasingly more
% abstract and robust features, and a deconvolutional pathway which consists of
% alternating deconvolutional and unpooling layers. In order for the segmentation
% to be driven by both low-level and high-level features, we introduce shortcut
% connections between layers of the two pathways. Similar to the u-net
% architecture, this allows the segmentation to be driven by both low-level and
% high-level features. In addition, the formalization of shortcut connections
% allows errors to be propagated through the shortcut connections, which in tern
% allows the learning of low-level features that are tuned for segmentation.

% Mention that we compared different architectures

% TODO: differences to u-net, contribution to u-net. U-net more for tissues, our
% method for brain, one structure and we are in 3D. What are new challenges in
% 3D? Runtime and memory.

We propose a new convolutional network architecture that combines the advantages
of a CEN \cite{brosch2015} and a u-net \cite{ronneberger2015}. Our
network is divided into two pathways, a traditional convolutional pathway, which
consists of alternating convolutional and pooling layers, and a deconvolutional
pathway, which consists of alternating deconvolutional and unpooling layers and
predicts the final segmentation. Similar to the u-net, we introduce shortcut
connections between layers of the two pathways. In contrast to the u-net, our
network uses deconvolution instead of upsampling in the expanding pathway and
predicts segmentations that have the same resolution as the input images and
therefore does not require special handling of the border regions. We have
evaluated our method on two widely used publicly available data sets for the
evaluation of MS lesion segmentation methods and a large in-house data set
from an MS clinical trial, with a comparison of network architectures of
different depths and with and without shortcuts\footnote{Where the risk of
confusion is minimal, we will refer to the shortcut connections between two
corresponding layers as a single shortcut (see Fig. 1).}. The results will be
presented in detail in Section~III.

% The results show that increasing depth from three to seven layers
% improves performance, and adding shortcut connections further improves accuracy.
% Overall, our method demonstrates consistently strong segmentation performance
% across a wide range of lesion loads, and in a direct comparison outperforms
% Lesion-TOADS \cite{shiee2010topology}, an established, widely used and freely
% available automatic MS lesion segmentation method.

% There has been a lot of interest in recent years in the machine learning
% community to develop better training methods for CNNs. However, training CNNs
% for medical image segmentation is particularly challenging due to the relatively
% small size of available training sets and the large size of 3D medical images,
% which only allows a few iterations to be used for training and hyperparameter
% tuning. Our second contribution is a comprehensive comparison of different first
% order and second order training method and parameter initialization strategies
% that can serve as a guideline for other researchers for training convolutional
% models for medical image segmentation.

% \item (Analysis of the relationship between regularization and training set
% size to avoid over- and underfitting)

% In this paper, we
% investigate the relationship between training set size and depth of the model on
% the segmentation performance and overfitting and compared the results to a the
% state-of-the-art lesion segmentation method lesion-TOADS, showing that network
% depth can improve segmentation performance for large data sets, but can also
% decrease the performance due to overfitting for smaller training sets, which
% makes network depth a tunable hyperparameter of the model. We've also
% investigated the influence of pre-training on the resulting model, showing that
% although the impact on the performance is minor, it has a big impact on the
% learned features.

% In this paper, we will build on our previous work. Although these networks have
% shown great potential, training of these networks can be difficult. The main
% contribution of this paper is a comprehensive comparison of different training
% strategies and algorithms that can serve as a guideline of how to design and
% train convolutional encoder networks. We have compared different first order and
% second order training methods and found the the right algorithm can have a major
% impact on the performance of the trained network. In addition, we evaluated the
% impact of pre-training and found that pre-training can significantly increase
% classification accuracy by preventing overfitting, if only small data sets are
% available. In addition, we evaluated the impact of the used objective function,
% comparing popular choices with our own objective function.

% We propose a new method for segmenting MS lesions that processes entire MRI
% volumes through a neural network with a novel objective function to
% automatically learn features tuned for lesion segmentation.
% Similar to fully convolutional networks \cite{kang2014fully}, our network
% processes entire volumes instead of patches, which removes the need to select
% representative patches, eliminates redundant calculations where patches overlap,
% and therefore scales up more efficiently with image resolution. This speeds up
% training and allows our model to take advantage of large data sets.
% Our neural network is 

% composed of three layers: an input layer composed of the
% image voxels of different modalities, a convolutional layer \cite{LeCun1998}
% that extracts features from the input layer at each voxel location, and a
% deconvolutional layer \cite{zeiler2011} that uses the extracted features to
% predict a lesion mask and thereby classify each voxel of the image in a single
% operation. The entire network is trained at the same time, which enables feature
% learning to be driven by segmentation performance. The proposed network is
% similar in architecture to a convolutional auto-encoder \cite{masci2011}, which
% produces a lower dimensional encoding of the input images and uses the decoder
% output to measure the reconstruction error needed for training, while our
% network uses the decoder to predict lesion masks of the input images. Due to the
% structural similarity to convolutional auto-encoders, we call our model a
% convolutional encoder network (CEN). Traditionally, neural networks are trained
% by back-propagating the sum of squared differences of the predicted and expected
% outputs. However, if one class is greatly underrepresented, as is the case for
% lesions, which typically comprise less than \SI{1}{\percent} of the image
% voxels, the algorithm would learn to ignore the minority class completely.
% To overcome this problem, we propose a new objective function based on a
% weighted combination of sensitivity and specificity, designed to deal with
% unbalanced classes and formulated to allow stable gradient computations.
