\section{Methods}
\label{sec:method}

\begin{figure*}[tb]
\centering
\input{tikzfigures/encoder_L2}

\caption{Pre-training and fine-tuning of the 7-layer convolutional encoder
network with shortcut that we used for our experiments. Pre-training is
performed on the input images using a stack of convolutional RBMs. The
pre-trained weights and bias terms are used to initialize a convolutional
encoder network, which is fine-tuned on pairs of input images, $x^{(0)}$, and
segmentations, $y^{(0)}$.}

\label{fig:network}
\end{figure*}

In this paper, the task of segmenting MS lesions is defined as finding a
function $s$ that maps multi-modal images $I$, e.g., $I = (I_\text{FLAIR},
I_\text{T1})$, to corresponding binary lesion masks $S$, where $1$ denotes a
lesion voxel and $0$ denotes a non-lesion voxel. Given a set of training images
$I_n$, $n \in \N$, and corresponding segmentations $S_n$, we model finding an
appropriate function for segmenting MS lesions as an optimization problem of the
following form
\begin{equation}
\hat{s} = \arg \min_{s \in \mathcal{S}} \sum_n E(S_n, s(I_n)),
\label{eq:segprob}
\end{equation}
where $\mathcal{S}$ is the set of possible segmentation functions, and $E$ is an
error measure that calculates the dissimilarity between ground truth
segmentations and predicted segmentations.

\subsection{Model Architecture}

The set of possible segmentation functions, $\mathcal{S}$, is modeled by the
convolutional encoder network with shortcut connections (CEN-s) illustrated in
Fig.~\ref{fig:network}. A CEN-s is a type of convolutional neural network (CNN)
\cite{LeCun1998} that is divided into two interconnected pathways, the
convolutional pathway and the deconvolutional \cite{zeiler2011} pathway. The
convolutional pathway consists of alternating convolutional and pooling layers.
The input layer of the convolutional pathway is composed of the image voxels
$x^{(0)}_i(\vect{p})$, $i \in [1, C]$, where $i$ indexes the modality or input
channel, $C$ is the number of modalities or channels, and $\vect{p} \in \N^3$
are the coordinates of a particular voxel. The convolutional layers
automatically learn a feature hierarchy from the input images. A convolutional
layer is a deterministic function of the following form
\begin{equation}
x^{(l)}_j = \max \Bigg(0, \sum_{i=1}^C\tilde{w}^{(l)}_{\text{c},ij}*x^{(l-1)}_i
+ b^{(l)}_j\Bigg),
\end{equation}
where $l$ is the index of a convolutional layer, $x^{(l)}_j$, $j \in [1,F]$,
denotes the feature map corresponding to the trainable convolution filter
$w^{(l)}_{\text{c},ij}$, $F$ is the number of filters of the current layer,
$b^{(l)}_j$ are trainable bias terms, $*$ denotes valid convolution, and
$\tilde{w}$ denotes a flipped version of $w$, i.e., $\tilde{w}(a) = w(-a)$. To
be consistent with the inference rules of convolutional restricted Boltzmann
machines (convRBMs) \cite{lee2009convolutional}, which are used for
pre-training, convolutional layers convolve the input signal with flipped filter
kernels, while deconvolutional layers calculate convolutions with non-flipped
filter kernels. We use rectified linear units \cite{nair2010} in all layers
except for the output layers, which have shown to improve the classification
performance of CNNs \cite{krizhevsky2012}. A convolutional layer is followed by
an average pooling layer \cite{scherer2010evaluation} that halves the number of
units in each dimension by calculating the average of each block of \num{2x2x2}
units per channel.

The deconvolutional pathway consists of alternating deconvolutional and
unpooling layers with shortcut connections to the corresponding
convolutional layers. The first deconvolutional layer uses the extracted
features of the convolutional pathway to calculate abstract segmentation
features
\begin{equation}
y^{(L-1)}_i = \max\Bigg(0, \sum_{j=1}^Fw^{(L)}_{\text{d},ij}\circledast
y^{(L)}_j + c^{(L-1)}_{i}\Bigg),
\end{equation}
where $y^{(L)} = x^{(L)}$, $L$ denotes the number of layers of the convolutional
pathway, $w^{(L)}_{\text{d},ij}$ and $c^{(L-1)}_i$ are trainable parameters of
the deconvolutional layer, and $\circledast$ denotes full convolution. To be
consistent with the general notation of deconvolutions \cite{zeiler2011}, the
non-flipped version of $w$ is convolved with the input signal.

Subsequent
deconvolutional layers use the activations of the previous layer
and corresponding convolutional layer to calculate more localized segmentation
features
% \begin{multline}
% y^{(l)}_i = \max\Bigg(0, \sum_{j=1}\Big(w^{(l+1)}_{\text{d},ij}\circledast
% y^{(l+1)}_j\\
% + w^{(l+1)}_{\text{s},ij}\circledast x^{(l+1)}_j\Big) +
% c^{(l)}_i\Bigg)
% \end{multline}
\begin{multline}
y^{(l)}_i = \max\Bigg(0, 
\sum_{j=1}^Fw^{(l+1)}_{\text{d},ij}\circledast y^{(l+1)}_j\\
+ \sum_{j=1}^F w^{(l+1)}_{\text{s},ij}\circledast x^{(l+1)}_j +
c^{(l)}_i\Bigg),
\end{multline}
where $l$ is the index of a deconvolutional layer with shortcut, and
$w^{(l+1)}_{\text{s},ij}$ are the shortcut filter kernels connecting the
activations of the convolutional pathway with the activations of the
deconvolutional pathway. The last deconvolutional layer integrates the low-level
features extracted by the first convolutional layer with the high-level features
from the previous layer to calculate a probabilistic lesion mask
\begin{equation}
y^{(0)}_1 = \sigm\Bigg(\sum_{j=1}^F\Big(w^{(1)}_{\text{d},1j}\circledast
y^{(1)}_j +
w^{(1)}_{\text{s},1j}\circledast x^{(1)}_j\Big) + c^{(0)}_1\Bigg),
\end{equation}
where we use the sigmoid function defined as $\sigm(z) = (1 + \exp(-z))^{-1}, z
\in \R$ instead of the rectified linear function in order to obtain a
probabilistic segmentation with values in the range between 0 and 1.
To produce a binary lesion mask from the probabilistic output of our model, we
chose a fixed threshold such that the mean Dice similarity coefficient
\cite{dice1945measures} is maximized on the training set and used the same
threshold for the evaluation on the test set.

\begin{comment}
The set of possible segmentation functions is modeled by the convolutional
encoder network illustrated in Fig.~\ref{fig:network}. Our network consists of
three layers: an input layer, a convolutional layer, and a deconvolutional
layer. The input layer is composed of the image voxels $x^{(1)}_i(\vect{p})$, $i
\in [1, C], C \in \N$, where $i$ indexes the modality, $C$ is the number of
modalities, and $\vect{p} \in \R^3$ are the coordinates of a particular voxel.
The convolutional layer automatically learns features from the input images. It
is a deterministic function of the following form
\begin{equation}
y^{(1)}_j = \max \left(0, \sum_{i=1}^{C}\tilde{w}^{(1)}_{ij}*x^{(1)}_i +
b^{(1)}_j\right)
\end{equation}
where $y^{(1)}_j, j \in [1, F], F \in \N$, denotes the feature map corresponding
to the trainable convolution filter $w^{(1)}_{ij}$, $F$ is the number of
filters, $b_j$ is a trainable bias term, $*$ denotes valid convolution, and
$\tilde{w}$ denotes a flipped version of $w$. The deconvolutional layer uses the
extracted features to calculate a probabilistic lesion mask as follows
\begin{equation}
y^{(2)} = \sigm\left(\sum_{j=1}^Fw^{(2)}_{j}\circledast x^{(2)}_j +
b^{(2)}\right)
\end{equation}
where $x^{(2)}_j = y^{(1)}_j$, $w^{(2)}_j$ and $b^{(2)}$ are trainable
parameters, $\circledast$ denotes full convolution, and $\sigm(z)$ denotes the
sigmoid function defined as $\sigm(z) = (1 + \exp(-z))^{-1}, z \in \R$. To
obtain a binary lesion mask from the probabilistic output of our model, we chose
a fixed threshold such that the mean Dice similarity coefficient is
maximized on the training set.
\end{comment}

% \begin{itemize}
% \item Parameters can be learned by minimizing the error $E$ on the training set
% \item Minimization requires calculation of gradient
% \item Derive the gradient, also with new notation, multiple layers and short
% cuts
% \end{itemize}

\subsection{Gradient Calculation}

The parameters of the model can be efficiently learned by minimizing the error
$E$ for each sample of the training set, which requires the calculation of the
gradient of $E$ with respect to the model parameters \cite{LeCun1998}.
Typically, neural networks are trained by minimizing the sum of squared
differences (SSD), which can be calculated for a single image as follows
\begin{equation}
% Error function
E = \frac{1}{2}\sum_{\vect{p}}\left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2,
\end{equation}
where $\vect{p} \in \N^3$ are the coordinates of a particular voxel.
The partial derivatives of the error with respect to the model parameters can be
calculated using the delta rule and are given by 
% $\Delta w^{(l)}_{\text{d},ij} =
% \delta^{(l-1)}_{\text{d},i} * \tilde{y}^{(l)}_j$, $\Delta w^{(l)}_{\text{s},ij}
% = \delta^{(l-1)}_{\text{d},i} * \tilde{x}^{(l)}_j$, $\Delta
% w^{(l)}_{\text{c},ij} = x^{(l-1)}_i * \tilde{\delta}^{(l)}_{\text{c},j}$
\begin{align}
\label{eq:dEd}
% Deconvolutional filters
\frac{\partial E}{\partial w^{(l)}_{\text{d},ij}} &=
\delta^{(l-1)}_{\text{d},i} * \tilde{y}^{(l)}_j, &
% Deconvolutional bias
\frac{\partial E}{\partial c^{(l)}_i} &= \sum_{\vect{p}}
\delta^{(l)}_{\text{d},i}(\vect{p}), \\
\label{eq:dEs}
% Shortcut filters
\frac{\partial E}{\partial w^{(l)}_{\text{s},ij}}
 &= \delta^{(l-1)}_{\text{d},i} * \tilde{x}^{(l)}_j, \\
 \label{eq:dEc}
% Convolutional filters
\frac{\partial E}{\partial w^{(l)}_{\text{c},ij}} 
&= x^{(l-1)}_i * \tilde{\delta}^{(l)}_{\text{c},j},\text{ and}&
% Convolutional bias
\frac{\partial E}{\partial b^{(l)}_i} &= \sum_{\vect{p}}
\delta^{(l)}_{\text{c},i}(\vect{p}).
\end{align}
For the first layer, $\delta^{(0)}_{\text{d},1}$ can be calculated by
\begin{equation}
% Delta update
\delta^{(0)}_{\text{d},1} = \big(y^{(0)}_1
-S\big)y^{(0)}_1\big(1-y^{(0)}_1\big).
\label{eq:delta0}
\end{equation}
The derivatives of the error with respect to the parameters of the other layers
can be calculated by applying the chain rule of partial derivatives, which
yields to
\begin{align}
\label{eq:deltad}
% Delta update of deconvolutional layer
\delta^{(l)}_{\text{d},j} &=
\big(\tilde{w}^{(l)}_{\text{d},ij}*\delta^{(l-1)}_{\text{d},i}\big)\I\big(y^{(l)}_j
> 0\big),\\
\label{eq:deltac}
% Delta update of convolutional layer
\delta^{(l)}_{\text{c},i} &=
\big(w^{(l+1)}_{\text{c},ij}\circledast\delta^{(l+1)}_{\text{c},j}\big)\I\big(x^{(l)}_i
> 0\big),
\end{align}
where $l$ is the index of a deconvolutional or convolutional layer,
$\delta^{(L)}_{\text{c},i} = \delta^{(L)}_{\text{d},j}$, and $\I(z)$ denotes the
indicator function defined as $1$ if the predicate $z$ is true and $0$
otherwise. If a layer is connected through a shortcut,
$\delta^{(l)}_{\text{c},j}$ needs to be adjusted by propagating the error back
through the shortcut connection. In this case, $\delta^{(l)}_{\text{c},j}$ is
calculated by
\begin{equation}
% Delta update convolutional with shortcut
\delta^{(l)}_{\text{c},j} =
\big(\delta^{(l)}_{\text{c},j}{}'+
\tilde{w}^{(l)}_{\text{s},ij}*\delta^{(l-1)}_{\text{d},i}\big)\I\big(x^{(l)}_j
> 0\big),
\label{eq:deltas}
\end{equation}
where $\delta^{(l)}_{\text{c},j}{}'$ denotes the activation of unit
$\delta^{(l)}_{\text{c},j}$ before taking the shortcut connection into account.

% Introduce new objective function and it's gradient

The sum of squared differences is a good measure of classification accuracy, if
the two classes are fairly balanced. However, if one class contains vastly more
samples, as is the case for lesion segmentation, the error measure is dominated
by the majority class and consequently, the neural network would learn to ignore
the minority class. To overcome this problem, we use a combination of
sensitivity and specificity, which can be used together to measure
classification performance even for vastly unbalanced problems. More precisely,
the final error measure is a weighted sum of the mean squared difference of the
lesion voxels (sensitivity) and non-lesion voxels (specificity), reformulated to
be error terms:
\begin{multline} 
E = r\frac{\textstyle\sum_{\vect{p}} \left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2 S(\vect{p})}{\textstyle\sum_{\vect{p}} S(\vect{p})}
\\
 + (1-r)\frac{\textstyle\sum_{\vect{p}} \left(S(\vect{p}) -
y^{(0)}(\vect{p})\right)^2 \big(1 - S(\vect{p})\big)}{%
\textstyle\sum_{\vect{p}}\big(1 - S(\vect{p})\big)}.
\end{multline}
We formulate the sensitivity and specificity errors as squared errors in order
to yield smooth gradients, which makes the optimization more robust. The
sensitivity ratio $r$ can be used to assign different weights to the two terms.
Due to the large number of non-lesion voxels, weighting the specificity error
higher is important, but based on preliminary experimental results \cite{brosch2015},
the algorithm is stable with respect to changes in $r$, which largely affects the
threshold used to binarize the probabilistic output. A detailed evaluation of
the impact of the sensitivity ratio on the learned model is presented in
Section~III-D.

To train our model, we must compute the derivatives of the modified objective
function with respect to the model parameters. Equations
\ref{eq:dEd}--\ref{eq:dEc} and \ref{eq:deltad}--\ref{eq:deltas} are a
consequence of the chain rule and independent of the chosen similarity measure.
Hence, we only need to derive the update rule for $\delta^{(0)}_{\text{d},1}$.
With $\alpha = 2r (\sum_{\vect{p}}S(\vect{p}))^{-1}$ and $\beta = 2(1 -
r)(\sum_{\vect{p}}(1 - S(\vect{p})))^{-1}$, we can rewrite $E$ as
\begin{align}
% \nonumber
% E =& \frac{1}{2}\sum_{\vect{p}}\Big(S(\vect{p})-y^{(0)}_1(\vect{p})\Big)^2 
% \alpha S(\vect{p})\\
% &+\frac{1}{2}\sum_{\vect{p}}\Big(S(\vect{p})-y^{(0)}_1(\vect{p})\Big)^2
% \beta(1-S(\vect{p}))\\
E=& \frac{1}{2}\sum_{\vect{p}}\big(\alpha S(\vect{p}) +
\beta(1-S(\vect{p}))\big)\Big(S(\vect{p})-y^{(0)}_1(\vect{p})\Big)^2.
\end{align}
%
Our objective function is similar to the SSD, with an additional multiplicative
term applied to the squared differences. The additional factor is constant with
respect to the model parameters. Consequently, $\delta^{(0)}_{\text{d},1}$ can
be derived analogously to the SSD case, and the new factor is simply carried
over:
\begin{equation} 
\delta^{(0)}_{\text{d},1} = \big(\alpha S + \beta (1 - S)\big)\big(y^{(0)}_1 -
S\big) y^{(0)}_1 \big(1 - y^{(0)}_1\big).
\end{equation}

% The update for $\delta^{(0)}_{\text{d},1}$ can be derived analogously to the SSD
% case, and is given by
% \begin{equation} 
% \delta^{(0)}_{\text{d},1} = \big(\alpha S + \beta (1 - S)\big)\big(y^{(0)}_1 -
% S\big) y^{(0)}_1 \big(1 - y^{(0)}_1\big)
% \end{equation}
% where $\alpha = 2r (\sum_{\vect{p}}S(\vect{p}))^{-1}$ and $\beta = 2(1 -
% r)(\sum_{\vect{p}}(1 - S(\vect{p})))^{-1}$.

\subsection{Training}
% Initialize training

At the beginning of the training procedure, the model parameters need to be
initialized and the choice of the initial parameters can have a big impact on
the learned model \cite{sutskever2013importance}. In our experiments, we found
that initializing the model using pre-training \cite{Hinton2006} on the input images was required in order
to be able to fine-tune the model using the ground truth segmentations without
getting stuck in an early local minimum. Pre-training can be performed layer by
layer \cite{Hinton2006b} using a stack of convRBMs (see Fig.~\ref{fig:network}),
thereby avoiding the potential problem of vanishing or exploding gradients
\cite{hochreiter1991untersuchungen}. The first convRBM is trained on the input
images, while subsequent convRBMs are trained on the hidden activations of the
previous convRBM. After all convRBMs have been trained, the model parameters of
the CEN-s can be initialized as follows (showing the first convolutional and
the last deconvolutional layers only, see Fig.~\ref{fig:network})
\begin{align}
w_{\text{c}}^{(1)} &= \hat{w}^{(1)}, &
w_{\text{d}}^{(1)} &= 0.5\hat{w}^{(1)}, &
w_{\text{s}}^{(1)} &= 0.5\hat{w}^{(1)} \\
b^{(1)} &= \hat{b}^{(1)}, &
c^{(0)} &= \hat{c}^{(1)},
\end{align}
where $\hat{w}^{(1)}$ are the filter weights, $\hat{b}^{(1)}$ are the hidden
bias terms, and $\hat{c}^{(1)}$ are the visible bias terms of the first convRBM.
% Alternatively, the bias terms can be initialized with zero and the
% filter weights are initialized using random values that are drawn from a
% distribution such that the variance of the activations of the layer units
% remains roughly the same throughout all layers. This can be achieved by drawing
% samples from the normal distribution $\mathcal{N}(0, \sigma)$, where $\sigma =
% \sqrt{2/N}$, and $N$ denotes the number of incoming connections for one unit.
% The influence of the initialization strategy on the learned model is analyzed in
% Section~??.

A major challenge for gradient-based optimization methods is the choice of an
appropriate learning rate. Classic stochastic gradient descent \cite{LeCun1998}
uses a fixed or decaying learning rate, which is the same for all parameters of
the model. However, the partial derivatives of parameters of different layers
can vary substantially in magnitude, which can require different learning rates.
In recent years, there has been an increasing interest in developing methods for
automatically choosing independent learning rates. Most methods (e.g., AdaGrad
\cite{duchi2011adaptive}, AdaDelta \cite{zeiler2012adadelta}, RMSprop
\cite{dauphin2015rmsprop}, and Adam \cite{kingma2014adam}) collect different
statistics of the partial derivatives over multiple iterations and use this
information to set an adaptive learning rate for each parameter. This is
especially important for the training of deep networks, where the optimal
learning rates often differ greatly for each layer. In our initial experiments,
networks obtained by training with AdaDelta, RMSprop, and Adam performed
comparably well, but AdaDelta was the most robust to the choice of
hyperparameters, so we used AdaDelta for all results reported.

% Second-order methods, like
% Hessian-free optimization [??], do not require a learning rate. Instead, a step
% towards the minimum of a quadratic approximation of the error function (or a
% semi positive definite approximation thereof) is performed in each iteration.
% Calculation of one update is much more costly for second-order methods, but it
% also performs much more progress and navigates valleys of ``pathological
% curvature'' more efficiently, which reduces the number of epochs required to
% train the model. For more details about the aforementioned methods, the reader
% is referred to the relevant papers.

\subsection{Implementation}

Pre-training and fine-tuning were performed using highly optimized
GPU-accelerated implementations of 3D convRBMs and CENs that performs training
in the frequency domain \cite{brosch2014efficient}. Our frequency domain
implementation significantly speeds up the training by mapping the calculation
of convolutions to simple element-wise multiplications, while adding only a
small number of Fourier transforms. This is especially important for the
training on 3D volumes, due to increased number of weights of 3D kernels
compared to 2D. Internal tests have shown that our frequency domain
implementation calculates the most time-consuming operations of the training
procedure 6 times faster than an implementation based on cuDNN
\cite{chetlur2014}, a library for calculating deep learning primitives, which is
used internally by many publicly available deep learning frameworks
\cite{jia2014,Bastien2012,collobert2011torch7}.
