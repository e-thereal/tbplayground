\sisetup{
  round-mode = places,
  round-precision = 3,
  exponent-product = \cdot,
  detect-weight=true,
  detect-inline-weight=math,
  tight-spacing = false,
  table-align-text-post = false
}%

\section{Experiments and Results}

To allow for a direct comparison to other state of the art lesion segmentation
methods, we have evaluated our method on the 20 labeled images from the MICCAI
lesion segmentation challenge. Our preprocessing pipeline includes downsampling
to \SI{1x1x1}{\milli\metre} resolution, global contrast normalization, brain
extraction. We have then cropped the images to a \num{159x203x153} region of
interest. To evaluate the performance, we have divided the data set into 5
splits with each split containing 16 images for training and 4 images for
testing such that each image will be used exactly once for testing. For all
experiments, we have used a filter size of \num{9x9x9}, 32 filters and trained
the model for 4000 epochs. Training took approximately 36 hours on a single
Geforce GTX 760 graphics card. Once the model is trained, segmentation of a
single image can be performed in approximately one second.\todo{Statistics
about lesion load (range, mean) for both data sets.}

To evaluate the impact of the training set size on the segmentation result, we
have evaluated our model on different subsets of a data set from an MS clinical
trail. With data set contained T2w and PDw images from 500 subjects at a single
time point. We have divided the data set into a training and test set containing
250 images each. We have then trained our model on a varying number of image
from the training set and evaluated to segmentation performance on both the
selected training samples and all images of the test set. The result of this
test is illustrated in Figure~\ref{fig:bioms}. If the training set size is too
small, features learned by our model are only representative for a subset of
lesions resulting on a relatively poor performance on the test set. As the
number of training samples is increased, the amount of overfitting is reduced
resulting in increasingly better performance on the test set.

% \paragraph{Training Pipeline}
% \begin{itemize}
% \item Downsample training images and training segmentations from
% \SI{0.5x0.5x0.5}{\milli\meter} to \SI{1x1x1}{\milli\meter} voxel resolution.
% \item Perform brain extraction
% \item Crop to smallest ROI
% \item Pad all images to standard size
% \item[$\Rightarrow$] Calculate combined cropping parameters
% \item Perform training of the NN on the downsampled training set
% \item Calculate probability maps for the entire downsampled training set
% \item Upsample the probability maps to the native resolution
% \item Crop lesion masks in native resolution to fit the same ROI as the
% upsampled probability masks
% \item Choose the threshold that maximizes the DSC in the training set 
% \end{itemize}

\paragraph{Experiments to come}

\begin{itemize}

\begin{figure}[tb]
\centering
\missingfigure{Good and bad examples including FLAIR, T2w, combined ground
truth and predicted segmentation with green TP, red FP, blue FN, black TN.}
\caption{Example segmentation using our method. Comment while the method
performed poorly. Maybe it just was a very difficult case.}
\end{figure}

\item Visualize segmentation performance. Show a good and a bad example.
  
\item Comparison with state-of-the-art methods. I will compare our method to
Geremia, dictionary learning and Souplet in terms of TPR, PPV, and DSC. Need to
check if I can get Tianming Zhan and  BRICQ St\'ephanie papers. Can't compare to
Xavier Tomas-Fernandez, because he didn't compute TPR, PPV, or DSC.

\item Sensitivity analysis. Will be performed with best parameters as baseline.
Parameters to be varied are: a) sensitivity ratio (0.05, 0.02, 0.01), b) number
of epochs for training, c) filter size (13, 11, 9, 7, 5), d) number of filters
(64, 32, 16), and e) number of training cases (2-fold, 3-fold, 4-fold, and
5-fold CV). In all cases, I will compare the influence on the training and
testing DSC and the best threshold.

\item Does the train performance correlate with the lesion load? Visualize good,
average and bad example. If it correlates with lesion load, visualize examples
with high, average, and low lesion load.

\item I need to get my challenge ranking. Inclusion of these results is
optional.

\item \emph{Optional}\quad Stride-1 tests: Requires memory efficient bias term
handling for shared bias terms. Might yield improved results and nicer filters
and I don't need to worry about explaining strided convolutions.

\item \emph{Optional}\quad Visualization of filters.

\item \emph{Optional}\quad Evaluation on BioMS using stride-1 convolutions and
new pre-processing pipeline with and without lesion prior, with individual and
shared bias terms. Metrics: TPR, PPV, DSC, correlations with lesion load and
clinical scores.

\end{itemize}

% The algorithm will be evaluated on the BioMS data set, some other internal data
% sets, the MICCAI 2008 MS Lesion Segmentation Challenge data set, and the ISBI
% 2015 MS Lesion Segmentation Challenge data set. We can't do much with BioMS,
% because it doesn't have FLAIR, which is required by other freely available
% methods. If we can get ground truth for data sets that have FLAIR, we can
% compare it against Lesion Toad and WMLS from UPenn. In any case, we can compare
% our method with other methods using the data provided by the segmentation
% challenges. We will use the following metrics: DSC, Sensitivity, Specificity,
% and PPV.

\begin{table}[htb]
\def\tabspace{12pt}
\sisetup{
  round-precision = 2,
}%
\caption{Comparison of state of the art methods with our method.}
\centering
\begin{tabular}{l%
@{\hspace{\tabspace}}S[table-format=2.2]
@{\hspace{\tabspace}}S[table-format=2.2]
@{\hspace{\tabspace}}S[table-format=2.2]
}
\toprule
Method & {TPR} & {PPV} & {DSC} \\ 
\midrule
Souplet et. al (2008) & 20.65 & 30.00 & {---} \\ 
Weiss et. al (2013) & 33.00 & 36.85 & 29.05 \\ 
Geremia et. al (2010) & 39.85 & 40.35 & {---}  \\
Our method & 39.71210 & 41.38274 & 35.52401 \\
%Our method & 41.49 & 36.90 & 34.20 \\
%Interleave & 41.81 & 35.48 & 34.79 \\
%Augmented  & 44.69 & 35.67 & 35.69 \\
%SenRatio   & 41.94 & 38.13 & 35.04 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{Segmentation results measured using the Dice Similarity Coefficient.
Two layer auto-encoder, stride size of \num{2x2x1}, 32 filters, sensitivity
ratio 0.05. Threshold finding methods: a) using a global threshold that
optimizes the average DSC of the entire data set, b) using the optimal
thresholds for each sample, and c) using predicted thresholds for each sample.}
\label{tab:segmentation}
\centering
\def\tabspace{10pt}
% \small
\begin{tabular}{@{}l%
@{\hspace{\tabspace}}S[table-format=1.3]%
@{\hspace{\tabspace}}S[table-format=1.3]
@{\hspace{\tabspace}}S[table-format=1.3]
@{\hspace{\tabspace}}S[table-format=1.3]
@{\hspace{\tabspace}}S[table-format=1.3]
@{\hspace{\tabspace}}S[table-format=1.3]
@{}}
\toprule
Method & \multicolumn{6}{c}{DSC per Lesion Load Category} \\
\addlinespace
 & {0.0--4.0} & {4.0--7.8} & {7.8--14.7} & {14.7--28.5} & {$>28.5$} &
{Average} \\
\midrule
Global threshold &  0.297805 & 0.545574 & 0.60051 & 0.650483 & 0.679614 &
 0.543181 \\
Optimal thresholds & 0.351185 & 0.568412 & 0.618115 & 0.667473 & 0.727397 &
0.572716 \\
Predicted thresholds & 0.337834 & 0.550028 & 0.587458 & 0.653475 &
0.716976 & 0.555662 \\
\bottomrule
\end{tabular}
\end{table}
% 
% \begin{table}
% \caption{Correlations with biomarkers is comparable to the expert
% segmentations. Threshold methods are: global threshold (LLG), optimal threshold
% (LLO), and predicted threshold (LLP).}
% \label{tab:cross}
% \centering
% \def\tabspace{14pt}
% \begin{tabular}{c%
% @{\hspace{\tabspace}}S[table-format=1.4]%
% @{\hspace{\tabspace}}S[table-format=2.6]%
% @{\hspace{\tabspace}}S[table-format=2.6]%
% @{\hspace{\tabspace}}S[table-format=2.6]%
% @{\hspace{\tabspace}}S[table-format=1.5]%
% @{\hspace{\tabspace}}S[table-format=1.6]%
% }
% \toprule
%      & {T25W} & {9-HPT} & {PASAT} & {MSFC} & {EDSS} & {T2LL} \\
% \midrule
% LLG & 0.0889204058874592 & -0.299046682079096*** & -0.35652944988915*** & -0.414587036427664*** & 0.111910491023784* & 0.867904019287295*** \\
% LLO & 0.0955077933828887* & -0.29002627967845*** & -0.36828512897955*** & -0.41898534832637*** & 0.116889743878327* & 0.988979555746338*** \\
% LLP & 0.0896915607291051 & -0.300773510817976*** & -0.342386015914701*** & -0.411524441008343*** & 0.134054634214833** & 0.869216107742852*** \\
% \addlinespace
% T2LL & 0.0830609699877066 & -0.288809520550953*** & -0.370891003440786*** &
% -0.415459123322053*** & 0.121219714387235* & {---} \\
% \bottomrule
% \end{tabular}
% \end{table}

